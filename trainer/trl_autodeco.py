from trl import SFTTrainer
from transformers.trainer import _is_peft_model
import torch.nn.functional as F
# import ipdb
import inspect
from typing import Any, Callable, Optional, TypeVar, Union
from datasets import Dataset, IterableDataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BaseImageProcessor,
    DataCollator,
    FeatureExtractionMixin,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    ProcessorMixin,
    Trainer,
    TrainingArguments,
    is_wandb_available,
)
from accelerate import PartialState
import warnings
from trl import SFTConfig
from trl.trainer.sft_trainer import remove_none_values
from trl.data_utils import (
    is_conversational,
    is_conversational_from_value,
    maybe_convert_to_chatml,
    pack_dataset,
    truncate_dataset,
)

class AutoDecoLLMTrainer(SFTTrainer):
    def __init__(
            self,
            *args,
            pad_token_id: int = None,
            temp_loss_weight: float = 1.0,
            temp_objective: str = "legacy_ce",
            min_p_ratio: float = 0.1,
            temp_hinge_weight: float = 1.0,
            temp_reg_weight: float = 0.0,
            easy_token_drop_prob: float = 0.6,
            top_p_loss_method: str = "soft",
            **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.pad_token_id = pad_token_id
        self.temp_loss_weight = temp_loss_weight
        self.temp_objective = temp_objective
        self.min_p_ratio = min_p_ratio
        self.temp_hinge_weight = temp_hinge_weight
        self.temp_reg_weight = temp_reg_weight
        self.easy_token_drop_prob = easy_token_drop_prob
        self.top_p_loss_method = top_p_loss_method


    def _prepare_dataset(
        self,
        dataset: Union[Dataset, IterableDataset],
        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],
        args: SFTConfig,
        packing: bool,
        formatting_func: Optional[Callable[[dict], str]],
        dataset_name: str,
    ) -> Union[Dataset, IterableDataset]:
        # Tabular backends like Arrow/Parquet insert `None` for mismatched keys in nested structures. Clean them from
        # sampled data.
        if isinstance(dataset, Dataset):  # IterableDataset does not support `with_transform`
            dataset = dataset.with_transform(remove_none_values)

        # If the dataset is already preprocessed (tokenized), skip the processing steps.
        column_names = list(next(iter(dataset)).keys())
        is_processed = "input_ids" in column_names

        # Build the kwargs for the `map` function
        map_kwargs = {}
        if isinstance(dataset, Dataset):  # IterableDataset does not support num_proc
            map_kwargs["num_proc"] = args.dataset_num_proc

        with PartialState().main_process_first():
            # Apply the formatting function if any
            if formatting_func is not None and is_processed:
                warnings.warn(
                    "You passed a dataset that is already processed (contains an `input_ids` field) together with a "
                    "formatting function. Therefore `formatting_func` will be ignored. Either remove the "
                    "`formatting_func` or pass a dataset that is not already processed.",
                    UserWarning,
                )

            if formatting_func is not None and not is_processed:
                if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                    map_kwargs["desc"] = f"Applying formatting function to {dataset_name} dataset"

                def _func(example):
                    return {"text": formatting_func(example)}

                try:
                    dataset = dataset.map(_func, batched=False, **map_kwargs)
                except Exception as e:
                    warnings.warn(
                        f"Failed to apply the formatting function due to the following error: {e}. This may be "
                        "because the function is designed for batched input. Please update it to process one example "
                        "at a time (i.e., accept and return a single example). For now, we will attempt to apply the "
                        "function in batched mode, but note that batched formatting is deprecated and will be removed "
                        "in version 0.21.",
                        DeprecationWarning,
                    )
                    dataset = dataset.map(_func, batched=True, **map_kwargs)

            if not is_processed:
                # Convert the dataset to ChatML if needed
                first_example = next(iter(dataset))
                if is_conversational_from_value(first_example):
                    if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                        map_kwargs["desc"] = f"Converting {dataset_name} dataset to ChatML"
                    column_names = next(iter(dataset)).keys()
                    dataset = dataset.map(
                        maybe_convert_to_chatml,
                        remove_columns="conversations" if "conversations" in column_names else None,
                        **map_kwargs,
                    )

                # Apply the chat template if needed
                first_example = next(iter(dataset))
                if not is_conversational(first_example):
                    if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                        map_kwargs["desc"] = f"Adding EOS to {dataset_name} dataset"

                    def add_eos(example, eos_token):
                        if "text" in example and not example["text"].endswith(eos_token):  # language modeling case
                            example["text"] = example["text"] + eos_token
                        elif "completion" in example and not example["completion"].endswith(eos_token):
                            example["completion"] = example["completion"] + eos_token
                        return example

                    dataset = dataset.map(
                        add_eos,
                        fn_kwargs={"eos_token": processing_class.eos_token},
                        remove_columns="messages" if "messages" in column_names else None,  # renamed to "text"
                        **map_kwargs,
                    )

                # Tokenize the dataset
                if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                    map_kwargs["desc"] = f"Tokenizing {dataset_name} dataset"

                def tokenize(example, processing_class, dataset_text_field, assistant_only_loss):
                    def _looks_like_chat_messages(value):
                        return (
                            isinstance(value, list)
                            and len(value) > 0
                            and all(
                                isinstance(turn, dict)
                                and isinstance(turn.get("role"), str)
                                and isinstance(turn.get("content"), str)
                                for turn in value
                            )
                        )

                    prompt_value = example.get("prompt")
                    completion_value = example.get("completion")
                    has_prompt_completion = prompt_value is not None and completion_value is not None

                    if has_prompt_completion:  # prompt-completion case
                        output = {}
                        if _looks_like_chat_messages(prompt_value) and _looks_like_chat_messages(completion_value):
                            prompt_ids = processing_class.apply_chat_template(
                                prompt_value,
                                tools=example.get("tools"),
                                **example.get("chat_template_kwargs", {}),
                            )
                            prompt_completion_processed = processing_class.apply_chat_template(
                                prompt_value + completion_value,
                                return_dict=True,
                                return_assistant_tokens_mask=assistant_only_loss,
                                tools=example.get("tools"),
                                **example.get("chat_template_kwargs", {}),
                            )
                            prompt_completion_ids = prompt_completion_processed["input_ids"]
                            if "assistant_masks" in prompt_completion_processed:
                                output["assistant_masks"] = prompt_completion_processed["assistant_masks"]
                        else:
                            # prompt = [{"role": "user", "content": example["prompt"]}]
                            # response = [{"role": "assistant", "content": example["completion"]}]
                            # prompt_ids = processing_class.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True)
                            # response_ids = processing_class.apply_chat_template(response, tokenize=True, add_generation_prompt=False)[3:-1]
                            # prompt_completion_ids = prompt_ids + response_ids
                            
                            prompt_text = prompt_value if isinstance(prompt_value, str) else str(prompt_value)
                            completion_text = completion_value if isinstance(completion_value, str) else str(completion_value)
                            prompt_ids = processing_class(text=prompt_text)["input_ids"][1:]
                            prompt_completion_ids = processing_class(text=prompt_text + completion_text)[
                                "input_ids"
                            ][1:]
                
                            # prompt_completion_ids = example['token_ids']
                        # Check if the tokenized prompt starts with the tokenized prompt+completion
                        if not prompt_completion_ids[: len(prompt_ids)] == prompt_ids:
                            warnings.warn(
                                "Mismatch between tokenized prompt and the start of tokenized prompt+completion. "
                                "This may be due to unexpected tokenizer behavior, whitespace issues, or special "
                                "token handling. Verify that the tokenizer is processing text consistently."
                            )

                        # Create a completion mask
                        completion_mask = [0] * len(prompt_ids) + [1] * (len(prompt_completion_ids) - len(prompt_ids))
                        output["input_ids"] = prompt_completion_ids
                        output["completion_mask"] = completion_mask
                    else:  # language modeling case
                        conversation = None
                        for conversation_key in (dataset_text_field, "messages", "conversations"):
                            if conversation_key in example and _looks_like_chat_messages(example[conversation_key]):
                                conversation = example[conversation_key]
                                break

                        if conversation is not None:
                            processed = processing_class.apply_chat_template(
                                conversation,
                                return_dict=True,
                                return_assistant_tokens_mask=assistant_only_loss,
                                tools=example.get("tools"),
                                **example.get("chat_template_kwargs", {}),
                            )
                            if "assistant_masks" in processed and 1 not in processed["assistant_masks"]:
                                raise RuntimeError(
                                    "You're using `assistant_only_loss=True`, but at least one example has no "
                                    "assistant tokens. This usually means the tokenizer's chat template doesn't "
                                    "generate assistant masks â€” it may be missing the `{% generation %}` keyword. Please "
                                    "check the template and ensure it's correctly configured to support assistant "
                                    "masking."
                            )
                            output = {k: processed[k] for k in ("input_ids", "assistant_masks") if k in processed}
                        else:
                            text_value = example.get(dataset_text_field)
                            if text_value is None:
                                raise KeyError(
                                    f"Missing dataset_text_field '{dataset_text_field}' for a non-conversational example. "
                                    f"Available keys: {list(example.keys())}"
                                )
                            if not isinstance(text_value, str):
                                raise ValueError(
                                    f"dataset_text_field '{dataset_text_field}' must be string for non-conversational "
                                    f"examples, got {type(text_value).__name__}."
                                )
                            output = {"input_ids": processing_class(text=text_value)["input_ids"]}
                    return output

                dataset = dataset.map(
                    tokenize,
                    fn_kwargs={
                        "processing_class": processing_class,
                        "dataset_text_field": args.dataset_text_field,
                        "assistant_only_loss": args.assistant_only_loss,
                    },
                    **map_kwargs,
                )

            # Pack or truncate
            if packing:
                if args.max_length is None:
                    raise ValueError("When packing is enabled, `max_length` can't be `None`.")
                if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                    map_kwargs["desc"] = f"Packing {dataset_name} dataset"

                columns = ["input_ids"]
                if "completion_mask" in dataset.column_names:
                    columns.append("completion_mask")
                if "assistant_masks" in dataset.column_names:
                    columns.append("assistant_masks")

                dataset = dataset.select_columns(columns)

                # Packing adds new column "seq_lengths" needed for document aware flash attention
                dataset = pack_dataset(dataset, args.max_length, args.packing_strategy, map_kwargs)
            elif args.max_length is not None:
                if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`
                    map_kwargs["desc"] = f"Truncating {dataset_name} dataset"
                dataset = truncate_dataset(dataset, args.max_length, map_kwargs)
            # For Liger kernel, ensure only the essential columns
            if args.use_liger_kernel:
                dataset = dataset.select_columns(
                    {"input_ids", "seq_lengths", "completion_mask"}.intersection(dataset.column_names)
                )

        return dataset
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            model_to_inspect = self.model
            if _is_peft_model(self.model):
                if hasattr(self.model, "get_base_model"):
                    model_to_inspect = self.model.get_base_model()
                else:
                    # PeftMixedModel do not provide a `get_base_model` method
                    model_to_inspect = self.model.base_model.model
            signature = inspect.signature(model_to_inspect.forward)
            self._signature_columns = list(signature.parameters.keys())
            # Labels may be named label or label_ids, the default data collator handles that.
            self._signature_columns += list(set(["label", "label_ids", "temp_labels", "top_p_labels", "top_k_labels", "completion_mask"] + self.label_names))

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        inputs['temp_loss_weight'] = self.temp_loss_weight
        inputs["temp_objective"] = self.temp_objective
        inputs["min_p_ratio"] = self.min_p_ratio
        inputs["temp_hinge_weight"] = self.temp_hinge_weight
        inputs["temp_reg_weight"] = self.temp_reg_weight
        inputs["easy_token_drop_prob"] = self.easy_token_drop_prob
        inputs["top_p_loss_method"] = self.top_p_loss_method
        outputs = model(**inputs)
        temp_loss = outputs.temp_loss.item() if outputs.temp_loss is not None else 0
        lm_loss = outputs.lm_loss.item() if outputs.lm_loss is not None else 0
        top_p_loss = outputs.top_p_loss.item() if outputs.top_p_loss is not None else 0
        self.log({"loss": outputs.loss.item(), "temp_loss": temp_loss, "lm_loss": lm_loss, "top_p_loss": top_p_loss})
        return outputs['loss']
