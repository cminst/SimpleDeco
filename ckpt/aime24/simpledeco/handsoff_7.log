INFO 02-15 21:58:11 [__init__.py:216] Automatically detected platform cuda.AutoDeco model registered with transformers (AutoConfig, AutoModel, AutoModelForCausalLM)INFO 02-15 21:58:14 [utils.py:328] non-default args: {'max_model_len': 32768, 'disable_log_stats': True, 'model': 'ckpt/simpledeco-qwen3-4b-thinking-merged'}INFO 02-15 21:58:21 [__init__.py:742] Resolved architecture: AutoDecoModelForCausalLM`torch_dtype` is deprecated! Use `dtype` instead!INFO 02-15 21:58:21 [__init__.py:1815] Using max model len 32768INFO 02-15 21:58:22 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.(EngineCore_DP0 pid=846) INFO 02-15 21:58:23 [core.py:654] Waiting for init message from front-end.(EngineCore_DP0 pid=846) INFO 02-15 21:58:23 [core.py:76] Initializing a V1 LLM engine (v0.1.dev9404+gaad21055a) with config: model='ckpt/simpledeco-qwen3-4b-thinking-merged', speculative_config=None, tokenizer='ckpt/simpledeco-qwen3-4b-thinking-merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=ckpt/simpledeco-qwen3-4b-thinking-merged, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}[W215 21:58:24.100046735 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0(EngineCore_DP0 pid=846) INFO 02-15 21:58:24 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0(EngineCore_DP0 pid=846) WARNING 02-15 21:58:24 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.(EngineCore_DP0 pid=846) INFO 02-15 21:58:24 [gpu_model_runner.py:2430] Starting to load model ckpt/simpledeco-qwen3-4b-thinking-merged...(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [gpu_model_runner.py:2462] Loading model from scratch...(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:149] ================================================================================(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:150] Initializing AutoDeco model for vLLM:(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:151]   - base_model_type: qwen3(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:152]   - use_enhanced_features: True(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:153]   - hidden_size: 2560(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:154] ================================================================================(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:165]   - Loading base model class: Qwen3ForCausalLM(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [cuda.py:362] Using Flash Attention backend on V1 engine.(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:201] ✓ AutoDeco model initialized successfully(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:202] ================================================================================(EngineCore_DP0 pid=846) INFO 02-15 21:58:25 [autodeco.py:348] Loading AutoDeco weights from merged checkpoint...Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.86it/s]Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.53it/s]Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.23it/s]Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.03it/s](EngineCore_DP0 pid=846) (EngineCore_DP0 pid=846) INFO 02-15 21:58:26 [autodeco.py:359] ✓ Successfully loaded 298 parameters(EngineCore_DP0 pid=846) INFO 02-15 21:58:26 [autodeco.py:366]   - Base model (llm.*): 290 parameters(EngineCore_DP0 pid=846) INFO 02-15 21:58:26 [autodeco.py:367]   - Temperature head (temp_head.*): 4 parameters(EngineCore_DP0 pid=846) INFO 02-15 21:58:26 [autodeco.py:368]   - Top-p head (top_p_head.*): 4 parameters(EngineCore_DP0 pid=846) INFO 02-15 21:58:26 [default_loader.py:268] Loading weights took 1.49 seconds(EngineCore_DP0 pid=846) INFO 02-15 21:58:27 [gpu_model_runner.py:2484] Model loading took 7.6094 GiB and 1.729909 seconds(EngineCore_DP0 pid=846) INFO 02-15 21:58:35 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/35217b2f70/rank_0_0/backbone for vLLM's torch.compile(EngineCore_DP0 pid=846) INFO 02-15 21:58:35 [backends.py:550] Dynamo bytecode transform time: 7.67 s(EngineCore_DP0 pid=846) INFO 02-15 21:58:41 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.247 s(EngineCore_DP0 pid=846) INFO 02-15 21:58:42 [monitor.py:34] torch.compile takes 7.67 s in total(EngineCore_DP0 pid=846) INFO 02-15 21:58:43 [gpu_worker.py:298] Available KV cache memory: 57.96 GiB(EngineCore_DP0 pid=846) INFO 02-15 21:58:43 [kv_cache_utils.py:864] GPU KV cache size: 422,016 tokens(EngineCore_DP0 pid=846) INFO 02-15 21:58:43 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 12.88xCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:03<00:00, 21.85it/s](EngineCore_DP0 pid=846) INFO 02-15 21:58:47 [gpu_model_runner.py:3215] Graph capturing finished in 4 secs, took 0.54 GiB(EngineCore_DP0 pid=846) INFO 02-15 21:58:47 [gpu_worker.py:391] Free memory on device (78.57/79.18 GiB) on startup. Desired GPU memory utilization is (0.9, 71.26 GiB). Actual usage is 7.61 GiB for weight, 5.63 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.54 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=61493640294` to fit into requested memory, or `--kv-cache-memory=69346530816` to fully utilize gpu memory. Current kv cache memory in use is 62229740646 bytes.(EngineCore_DP0 pid=846) INFO 02-15 21:58:47 [core.py:218] init engine (profile, create kv cache, warmup model) took 20.09 secondsINFO 02-15 21:58:48 [llm.py:295] Supported_tasks: ['generate']INFO 02-15 21:58:48 [__init__.py:36] No IOProcessor plugins requested by the modelAdding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 635.80it/s]Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████| 480/480 [2:07:41<00:00, 15.96s/it, est. speed input: 8.04 toks/s, output: 1186.25 toks/s]Overall avg Acc: 56.67%ERROR 02-16 00:06:40 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.